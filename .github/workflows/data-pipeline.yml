name: Data Pipeline - Scheduled Updates

on:
  schedule:
    # Run every 2 hours
    - cron: '0 */2 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write

env:
  PYTHON_VERSION: '3.10'

jobs:
  fetch-and-train:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure required directories exist
        run: |
          mkdir -p data/raw data/processed models

      - name: Configure environment variables
        env:
          TWELVE_DATA_API_KEY: ${{ secrets.TWELVE_DATA_API_KEY }}
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          echo "Environment configured with secrets"

      - name: Setup DVC
        uses: iterative/setup-dvc@v1

      - name: Configure DVC remote auth (Dagshub)
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # Check if dagshub remote exists, add if not
          if ! dvc remote list 2>/dev/null | awk '{print $1}' | grep -qx 'dagshub'; then
            echo "Adding DVC remote 'dagshub'..."
            dvc remote add -d dagshub \
              "https://dagshub.com/$DAGSHUB_USERNAME/Real-Time-MLOps-Pipeline-for-USD-Forecasting.dvc"
          fi
          # Configure auth
          dvc remote modify dagshub --local auth basic
          dvc remote modify dagshub --local user "$DAGSHUB_USERNAME"
          dvc remote modify dagshub --local password "$DAGSHUB_TOKEN"
          echo "DVC remote configured successfully"

      - name: Extract data from API
        env:
          TWELVE_DATA_API_KEY: ${{ secrets.TWELVE_DATA_API_KEY }}
        run: |
          python -c "
          from src.data.data_extraction import TwelveDataClient
          from src.utils.logger import get_logger
          from datetime import datetime

          logger = get_logger('data_extraction_job')
          logger.info('Starting data extraction...')

          client = TwelveDataClient()
          df = client.fetch_to_dataframe()

          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          filepath = client.save_raw_data(df, timestamp=timestamp)

          logger.info(f'Data extracted and saved to {filepath}')
          print(f'✓ Extracted {len(df)} records')
          "

      - name: Transform data
        run: |
          python -c "
          from src.data.data_transformation import DataTransformer
          from src.utils.logger import get_logger
          from pathlib import Path

          logger = get_logger('data_transformation_job')
          logger.info('Starting data transformation...')

          transformer = DataTransformer()
          processed_files = sorted(Path('data/raw').glob('raw_data_*.csv'))

          if processed_files:
              df_processed = transformer.transform(str(processed_files[-1]))
              logger.info('Data transformation completed')
              print(f'✓ Transformed data with {df_processed.shape[1]} features')
          else:
              raise FileNotFoundError('No raw data files found')
          "

      - name: Train model
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
        run: |
          python src/models/production_trainer.py

      - name: Version artifacts with DVC
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          set -e
          echo "=== Starting DVC versioning ==="

          # Find latest processed data file
          latest_processed=$(ls -1t data/processed/processed_data_*.parquet 2>/dev/null | head -n 1 || true)
          if [ -z "$latest_processed" ]; then
            echo "No processed parquet found to version" >&2
            exit 1
          fi
          echo "Found processed data: $latest_processed"

          # Function to safely add file to DVC
          safe_dvc_add() {
            local file="$1"
            if [ -f "$file" ]; then
              echo "Adding $file to DVC..."
              # Remove existing .dvc file if it exists (to allow re-adding)
              rm -f "${file}.dvc" 2>/dev/null || true
              dvc add "$file" && echo "✓ Added $file" || echo "⚠ Failed to add $file"
            else
              echo "⚠ File not found, skipping: $file"
            fi
          }

          # Add artifacts to DVC (only if they exist)
          safe_dvc_add "$latest_processed"
          safe_dvc_add "models/latest_model.pkl"
          safe_dvc_add "models/scaler.pkl"
          safe_dvc_add "models/latest_metadata.json"

          # Push artifacts to remote (continue on error for individual files)
          echo "=== Pushing to DVC remote ==="
          dvc push -v 2>&1 || {
            echo "⚠ DVC push had issues, attempting to push individual files..."
            # Try to push each file individually
            for dvc_file in $(find . -name "*.dvc" -type f 2>/dev/null); do
              echo "Pushing $dvc_file..."
              dvc push "$dvc_file" 2>&1 || echo "⚠ Could not push $dvc_file"
            done
          }
          echo "=== DVC versioning complete ==="

      - name: Commit and push changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          # Add .dvc files and .gitignore changes
          git add "*.dvc" .gitignore data/.gitignore models/.gitignore 2>/dev/null || true
          git add data/**/*.dvc models/**/*.dvc 2>/dev/null || true

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: automated data update and model training [skip ci]"
            git push origin HEAD
          fi
        continue-on-error: true

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            models/latest_model.pkl
            models/latest_metadata.json
            models/scaler.pkl
          retention-days: 30

      - name: Send notification on failure
        if: failure()
        run: |
          echo "Pipeline failed! Check the logs for details."
          # Add Slack/email notification here if needed
