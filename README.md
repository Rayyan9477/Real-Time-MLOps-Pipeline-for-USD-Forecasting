# USD Volatility Prediction - Real-Time MLOps Pipeline

A production-grade MLOps pipeline for real-time USD volatility forecasting using EUR/USD forex data. This project demonstrates end-to-end ML lifecycle management with automated data ingestion, model training, deployment, and monitoring.

[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Project Overview

### Problem Statement
Predict next-hour USD volatility using EUR/USD forex pair data with hourly granularity. The system automatically adapts to concept drift (market regime changes) through continuous monitoring and retraining.

### Key Features
- âœ… **Automated Data Pipeline**: Airflow DAG for ETL with quality gates
- âœ… **Feature Engineering**: Lag features, rolling statistics, time encodings
- âœ… **Experiment Tracking**: MLflow integration with PostgreSQL + MinIO
- âœ… **Data Versioning**: DVC with Google Drive remote storage
- âœ… **CI/CD Pipeline**: GitHub Actions with CML for model comparison
- âœ… **Model Serving**: FastAPI REST API with Prometheus metrics
- âœ… **Monitoring**: Grafana dashboards with drift detection and alerts
- âœ… **Containerization**: Docker deployment ready for production

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Twelve Data    â”‚
â”‚      API        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow DAG    â”‚ â—„â”€â”€ Scheduled Daily
â”‚   (ETL Pipeline)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Extract      â”‚ â†’ Quality Checks
â”‚ 2. Transform    â”‚ â†’ Feature Engineering
â”‚ 3. Load         â”‚ â†’ MinIO Storage
â”‚ 4. Version      â”‚ â†’ DVC + Google Drive
â”‚ 5. Log          â”‚ â†’ MLflow Artifacts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Training â”‚
â”‚   (train.py)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ XGBoost       â”‚
â”‚ â€¢ TimeSeriesSplitâ”‚
â”‚ â€¢ Drift Detectionâ”‚
â”‚ â€¢ MLflow Trackingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLflow Registryâ”‚ â—„â”€â”€ PostgreSQL + MinIO
â”‚  (Production)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Serviceâ”‚
â”‚   (Docker)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ /predict      â”‚
â”‚ â€¢ /health       â”‚
â”‚ â€¢ /metrics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prometheus    â”‚ â”€â”€â–º â”‚    Grafana      â”‚
â”‚   (Metrics)     â”‚     â”‚  (Dashboard)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Docker & Docker Compose
- Git
- Twelve Data API key ([Get free key](https://twelvedata.com/register))
- Google Drive account for DVC storage ([Setup guide](DVC_SETUP.md))

### 1. Clone Repository
```bash
git clone https://github.com/Rayyan9477/Real-Time-MLOps-Pipeline-for-USD-Forecasting.git
cd Real-Time-MLOps-Pipeline-for-USD-Forecasting
```

### 2. Environment Setup
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
```

### 3. Configure Environment Variables
Edit `.env` file with your credentials:
```bash
# Twelve Data API
TWELVE_DATA_API_KEY=your_api_key_here

# Dagshub
DAGSHUB_REPO_OWNER=your_username
DAGSHUB_TOKEN=your_dagshub_token
MLFLOW_TRACKING_URI=https://dagshub.com/your_username/Real-Time-MLOps-Pipeline-for-USD-Forecasting.mlflow

# Docker Hub (for deployment)
DOCKER_USERNAME=your_docker_username
```

### 4. Initialize DVC with Google Drive
```bash
# Install DVC with Google Drive support
pip install dvc[gdrive]

# Initialize DVC
dvc init

# Add Google Drive remote
# Replace FOLDER_ID with your Google Drive folder ID
dvc remote add -d gdrive gdrive://FOLDER_ID

# Configure authentication (interactive)
dvc remote modify gdrive gdrive_use_service_account false

# For CI/CD, use service account (see DVC_SETUP.md)
```

For detailed DVC setup with Google Drive, see **[DVC_SETUP.md](DVC_SETUP.md)**.

### 5. Start Infrastructure
```bash
# Start Airflow, MinIO, Prometheus, Grafana
docker-compose up -d

# Check services
docker-compose ps
```

**Access Services:**
- Airflow UI: http://localhost:8080 (airflow/airflow)
- MLflow UI: http://localhost:5000 (no auth)
- MinIO Console: http://localhost:9001 (minioadmin/minioadmin)
- Prometheus: http://localhost:9090 (no auth)
- Grafana: http://localhost:3000 (admin/admin)
- FastAPI Docs: http://localhost:8000/docs (no auth)

**ğŸ“Š View Dashboards:**
- **Airflow**: Monitor ETL pipeline execution and DAG status
- **MLflow**: Track experiments, compare models, manage model registry
- **Grafana**: Real-time metrics, alerts, and performance monitoring
  - USD Volatility Prediction Monitoring (`usd-volatility-monitoring`)
  - MLOps Pipeline Overview (`mlops-pipeline-overview`)

For detailed dashboard configuration and usage, see **[DASHBOARD_ACCESS_GUIDE.md](DASHBOARD_ACCESS_GUIDE.md)**.

## ğŸ“Š Usage

### Run ETL Pipeline
```bash
# Trigger Airflow DAG manually
curl -X POST "http://localhost:8080/api/v1/dags/usd_volatility_etl_pipeline/dagRuns" \
  -H "Content-Type: application/json" \
  -u "airflow:airflow" \
  -d '{"conf":{}}'
```

### Train Model
```bash
# Train with default hyperparameters
python src/models/train.py

# Train with custom hyperparameters
python src/models/train.py --n-estimators 150 --max-depth 7 --learning-rate 0.05
```

### Start Prediction API
```bash
# Run locally
uvicorn src.api.app:app --host 0.0.0.0 --port 8000

# Or with Docker
docker build -t usd-volatility-predictor .
docker run -p 8000:8000 --env-file .env usd-volatility-predictor
```

### Make Predictions
```bash
# Test health endpoint
curl http://localhost:8000/health

# Make prediction
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "features": {
      "close_lag_1": 1.0854,
      "close_rolling_mean_24": 1.0850,
      "close_rolling_std_24": 0.0015,
      "hour_sin": 0.5,
      "hour_cos": 0.866,
      "log_return": 0.0002
    }
  }'

# Check metrics
curl http://localhost:8000/metrics
```

## ğŸ”„ CI/CD Workflow

### Branch Strategy
```
feature â†’ dev â†’ test â†’ master
          â†“       â†“        â†“
       Lint    Train   Deploy
       Test     CML    Docker
```

### GitHub Actions Workflows

1. **Feature â†’ Dev**: Code quality checks (Black, Flake8, Pylint, PyTest)
2. **Dev â†’ Test**: Full training pipeline + CML metric comparison
3. **Test â†’ Master**: Docker build, push to registry, deployment verification

### Setting Up CI/CD
Add GitHub Secrets:
```
DAGSHUB_TOKEN
DAGSHUB_USERNAME
MLFLOW_TRACKING_URI
MLFLOW_TRACKING_USERNAME
MLFLOW_TRACKING_PASSWORD
TWELVE_DATA_API_KEY
DOCKER_USERNAME
DOCKER_PASSWORD
```

## ğŸ“ˆ Monitoring & Observability

### Grafana Dashboard
Access: http://localhost:3000

**Panels:**
- Prediction latency (avg, P95, P99)
- Request rate
- Data drift ratio
- Error rate
- Total predictions

**Alerts:**
- High latency (>500ms)
- High drift (>20%)

### Prometheus Metrics
- `predictions_total`: Total predictions made
- `prediction_latency_seconds`: Prediction latency histogram
- `data_drift_ratio`: Current drift ratio
- `prediction_errors_total`: Total errors

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/unit/ -v

# Run with coverage
pytest tests/unit/ --cov=src --cov-report=html

# Run integration tests
pytest tests/integration/ -v
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ etl_dag.py              # ETL orchestration
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ prometheus.yml              # Prometheus config
â”‚   â””â”€â”€ grafana/                    # Grafana dashboards
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ extraction.py           # Data fetching & validation
â”‚   â”‚   â””â”€â”€ transformation.py       # Feature engineering
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ train.py                # Model training
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py                  # FastAPI service
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py               # Logging utilities
â”‚       â””â”€â”€ storage.py              # MinIO client
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                       # Unit tests
â”‚   â””â”€â”€ integration/                # Integration tests
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                  # CI/CD pipelines
â”œâ”€â”€ docker-compose.yml              # Infrastructure stack
â”œâ”€â”€ Dockerfile                      # API container
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Orchestration | Apache Airflow |
| Data Versioning | DVC + Dagshub |
| Experiment Tracking | MLflow + Dagshub |
| Model Training | XGBoost, scikit-learn |
| API Framework | FastAPI |
| Monitoring | Prometheus + Grafana |
| Containerization | Docker |
| CI/CD | GitHub Actions + CML |
| Object Storage | MinIO |
| Data Source | Twelve Data API |

## ğŸ“Š Model Performance

**Typical Metrics (EUR/USD Hourly Volatility):**
- RMSE: ~0.0008 - 0.0012
- MAE: ~0.0005 - 0.0008
- RÂ²: 0.65 - 0.75
- MAPE: 15-25%

*Note: Performance varies based on market conditions and training data.*

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request to `dev` branch

## ğŸ“ License

This project is licensed under the MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- [Twelve Data](https://twelvedata.com/) for forex data API
- [Dagshub](https://dagshub.com/) for MLOps platform
- [Apache Airflow](https://airflow.apache.org/) for orchestration
- [MLflow](https://mlflow.org/) for experiment tracking

## ğŸ“§ Contact

**Rayyan** - [GitHub](https://github.com/Rayyan9477)

**Project Link**: https://github.com/Rayyan9477/Real-Time-MLOps-Pipeline-for-USD-Forecasting

---

**â­ Star this repo if you find it helpful!**
